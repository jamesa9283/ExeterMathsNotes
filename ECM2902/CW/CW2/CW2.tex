\documentclass{article}


% Packages
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{float}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{shapes.geometric, positioning, arrows, intersections}
\tikzset{point/.style={circle,draw=black,inner sep=0pt,minimum size=3pt}}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}
\usepackage{fancyhdr}

\renewcommand{\ker}{\operatorname{Ker}}
\newcommand{\range}{\operatorname{Range}}
\newcommand{\Null}{\operatorname{null}}



% Macros
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\sub}{\subset}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\uvec}[1]{\hat{\underline{\textbf{#1}}}}
\newcommand{\veci}{\bm{\hat{\imath}}}
\newcommand{\vecj}{\bm{\hat{\jmath}}}
\newcommand{\veck}{\bm{\hat{k}}}
\newcommand{\vecn}{\underline{\mathbf{\hat{n}}}}
\newcommand{\e}{\varepsilon}
\renewcommand{\th}{\vartheta}
\newcommand{\de}{\delta}
\renewcommand{\k}{\kappa}
\newcommand{\p}{\Phi}
\renewcommand{\a}{\alpha}
\renewcommand{\b}{\beta}
\newcommand{\pa}{\partial}
\newcommand{\kd}{\delta_{i, j}}
\newcommand{\at}{\e_{i, j, k}}
\newcommand{\nab}{\underline{\nabla}}
\newcommand{\grad}{{\nab}\, f}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\fd}[2]{\frac{d #1}{d #2}}
\renewcommand{\div}{\nab \cdot}
\newcommand{\curl}{\nab \times}
\newcommand{\br}{\underline{\varrho}}
\newcommand{\tbt}[4]{\begin{pmatrix} #1 & #2 \\ #3 & #4\end{pmatrix}}
\newcommand{\vect}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\newcommand{\vecth}[3]{\begin{pmatrix} #1 \\ #2 \\ #3 \end{pmatrix}}
\newcommand{\thbth}[9]{\begin{pmatrix} #1 & #2 & #3 \\ #4 & #5 & #6 \\ #7 & #8 & #9\end{pmatrix}}
\newcommand{\iid}{\thbth 1 0 0 0 1 0 0 0 1}
\newcommand{\id}{\tbt 1 0 0 1}
\renewcommand{\l}{\lambda}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\rank}{rank}


%\setlength{\columnsep}{20pt}

%ToC stuff
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}

 % Document stuff

\title{Linear Algebra Coursework 2}
\author{James Arthur - 690055793}

\begin{document}
\maketitle

\begin{problem}
  Let $T_A : \R^3 \to \R^4$ be defined by $T_A(\vec v) = A\vec v$ where $A$ is the matrix,
  $$A = \begin{pmatrix}
      1 & -2 & -2\\
      2 & -4 & 1\\
      0 & 0 & 3 \\
      -1 & 2 & 0 \\
    \end{pmatrix}$$
  \begin{enumerate}
    \item Find a basis for $\ker(T_A)$ and a basis for $\range(T_A)$.
    \item Hence verify the Rank-Nullity Theorem holds for $T_A$.
  \end{enumerate}
\end{problem}

\begin{solution}
  We can take $A$ and preform row reduction and get the reduced row echelon form,
  $$ A_{rref} = \begin{pmatrix}
    1 & -2 & 0\\
    0 & 0 & 1 \\
    0 & 0 & 0 \\
    0 & 0 & 0 \\
  \end{pmatrix} $$
  and so we read off the solutions and get that,
  \begin{align*}
    x_1 - 2x_2 &= 0\\
    x_3 &= 0
  \end{align*}
  and hence we let $x_2 = a$, $a \in \R$. Then we can easily write the kernel of $A$ is,
  $$ \ker(T_A) = \left \{ \begin{pmatrix}
    2a \\ a \\ 0
  \end{pmatrix} : a \in \R \right\} $$
  We can now go back to the $A_{rref}$ and note the fact that there isn't a pivot in column 2 of the matrix. This tells us for the range we can remove the second column vector of the matrix from the column space as it isn't linearly independent. Hence we get the range as,
  $$ \range(T_A) = \spn \left(\left[\begin{pmatrix}
    1 \\ 2 \\ 0 \\ -1
  \end{pmatrix}, \begin{pmatrix}
    -2 \\ 1 \\ 3 \\ 0
  \end{pmatrix}\right]\right) $$

  \noindent
  (ii) \quad Now we can verify the Rank-Nullity theorem, after knowing that the $\rank$ and $\Null$ are just the dimensions of the $\range$ and $\ker$ respectively,
  \begin{align*}
    \dim V &= \rank T_A + \Null T_A \\
    &= \dim\range T_A + \dim\ker T_A \\
    &= 2 + 1 = 3\\
  \end{align*}
  and as we know that $V = \R^3$, then $\dim V = 3$. Hence verified.
\end{solution}

\newpage
\begin{problem}
  Are the following true or false? Justify your ‘true’ answers using the Rank-Nullity Theorem and ‘false’ answers with counterexamples.
  \begin{enumerate}
    \item An $n \times n$ matrix is always surjective.
    \item A $ 5 \times 4$ matrix can never be surjective.
    \item A $5 \times 4$ matrix can never be injective.
  \end{enumerate}
\end{problem}
\begin{solution}
  (i) \quad False, Corrolary 8.13 says that if an $n\times n$ matrix is injective it's surjective and if it's surjective it's injective. Hence a square matrix is bijective or neither. We need to find a matrix that isn't bijective. Keeping it simple take,
  $$ B = \begin{pmatrix}
    0
  \end{pmatrix} $$
  This has a rank of $0$, which is not equal to $1$. Hence it is not injective and so not surjective, however it is an $n\times n$ matrix. So a square matrix is not necessarily surjective. We can also say that $\Null B \ne 0$ and so not surjective (which is slightly more direct).\\

  \noindent
  (ii) \quad True, We have a $n \times m$ matrix and so $m > n$, this means, by Corrolary 8.13, this is true. The proof is as such for the general case, If $n < m$, then
\begin{align*}
  \rank T = \dim V - \Null T\\
  &= n - \Null T\\
  &\le n\\
  &< m = \dim W\\
\end{align*}
Hence $T$ cannot be surjective (by Corollary 8.10).\\

\noindent
(iii) \quad False, take the following $5 \times 4$ matrix,
$$ C = \begin{pmatrix}
  1 & 0 & 0 & 0\\
  0 & 1 & 0 & 0\\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 0
\end{pmatrix} $$
This has the following kernel,
$$ \ker C = \left\{ \begin{pmatrix}
  0 \\ 0 \\ 0 \\ 0
\end{pmatrix} \right\} $$
which then has dimension $0$. Hence $\null C = 0$ which then implies it's injective, by Corollary 8.10.
\end{solution}

\newpage
\begin{problem}
  Consider the linear transformation $T : P_2(\R) \to \R^3$ defined by
  $$ T(p) = \begin{pmatrix}
    p(1) \\ p(2) \\ p(3)
  \end{pmatrix} $$
  \begin{enumerate}
    \item Show that T is bijective.
    \item Using the result in (i), find the unique polynomial $q \in P_2(\R)$ that passes through the coordinates $(1, 3),(2, 5),(3, -1)$; why must this polynomial be unique?
  \end{enumerate}
\end{problem}

\begin{solution}
  We start by plugging in the basis vectors for $P_2(\R)$ into the linear transformation and providing a matrix that we can work off of.
  $$ T(p_0) = \begin{pmatrix}
    1 \\ 1 \\ 1
  \end{pmatrix} \qquad T(p_1) = \begin{pmatrix}
    1 \\ 2 \\ 3
  \end{pmatrix} \qquad T(p_2) = \begin{pmatrix}
    1 \\ 4 \\ 9
  \end{pmatrix}$$
  Hence, we can write a matrix down,
  $$ [T]_E^P = \begin{pmatrix}
    1 & 1 & 1 \\
    1 & 2 & 4 \\
    1 & 3 & 9
  \end{pmatrix} $$
  (i) \quad Let us look at the row reduced form of $[T]_E^P$,
  $$ \iid $$
  This matrix has a pivot in every row and column, hence it is bijective (by Remark 8.15).\\

  % For $[T]_P^E$ to be bijective, we need $\dim P_2(\R) = \dim \R^3$, which we do have as $P_2(\R) \cong \R^3$ (by Theorem 8.19). Hence by the definition of an isomorphism, $T$ is bijective.

  \noindent
  (ii) \quad Let $P$ be the basis for $\R^3$ and $E$ be the basis for $P_2(\R)$. We now want to calculate $[T]_E^P [\vec v]_P$, we can find $[T]_P^E$ by $([T]_E^P)^{-1}$ (by Corollary 7.13 and some fiddling). Hence,
  \begin{align*}
    [\vec v]_E &= \begin{pmatrix}
      1 & 1 & 1 \\
      1 & 2 & 4 \\
      1 & 3 & 9
  \end{pmatrix}^{-1}
  \begin{pmatrix}
    3 \\ 5 \\ -1
  \end{pmatrix}\\
  &= \begin{pmatrix}
    -7 \\ 14 \\ -4
\end{pmatrix}
  \end{align*}
  and hence, $p(x) = -7 + 14x - 4x^2$. This is unique as $[T]_E^P$ is bijective.
\end{solution}

\newpage
\begin{problem}
  Consider the following set of coupled real linear difference equations:
  \begin{align*}
    x_1(n) &= \frac{7}{5}x_1(n - 1) - \frac{1}{5}x_2(n - 1)\\
    x_2(n) &= \frac{4}{5}x_1(n- 1) + \frac{2}{5}x_2(n - 1)
  \end{align*}
  \begin{enumerate}
    \item Use diagonalisation to solve the system for arbitrary initial conditions $ x_1(0) = a$ and $x_2(0) = b$.
    \item Assuming $b \ne 4a$, what is the long term ratio $x_1 : x_2$?
    \item Assuming $b \ne 4a$, what are the long term growth rates of $x_1 and x_2$?
  \end{enumerate}
\end{problem}

\begin{solution}
  We can write the system as,
  $$ \begin{pmatrix}
    x_1(n) \\ x_2(n)
  \end{pmatrix} \begin{pmatrix}
    1.4 & -0.2\\ 0.8 & 0.4
  \end{pmatrix} \begin{pmatrix}
    x_1(n-1) \\ x_2(n-1)
  \end{pmatrix}$$
  Then we can find the eigenvectors of the matrix in the system,
  \begin{align*}
    |A - \l I| &= \left|\begin{pmatrix}
      \frac{7}{5} - \l & -\frac{1}{5}\\ \frac{4}{5} & \frac{2}{5} - \l
  \end{pmatrix}\right|\\
  &= \left( \frac{7}{5} - \l \right)\left( \frac{2}{5} - \l \right) + \frac{4}{25}\\
  &= \left( \frac{6}{5} - \l \right)\left( \frac{3}{5} - \l \right)
  \end{align*}
  Hence we have eigenvalues of $\l_1 = \frac{6}{5}$ and $\l_2 = \frac{3}{5}$. We can now find the eigenvectors of, $\displaystyle{\underline{x}_1 = \begin{pmatrix} 1 \\ 1
\end{pmatrix}}$ and $\displaystyle{\underline{x}_2 = \begin{pmatrix}
    \frac{1}{4} \\ 1
  \end{pmatrix}}$. Now by the method shown in the notes, we need to find $A^n$ and multiply it by the initial conditions and we have the solution, and so,
  \begin{align*}
    \vec{x}(n) &= \begin{pmatrix}
      1 & \frac{1}{4} \\ 1 & 1
  \end{pmatrix}\begin{pmatrix}
    \frac{6}{5}^n & 0 \\ 0 & \frac{3}{5}^n
\end{pmatrix}\begin{pmatrix}
  1 & \frac{1}{4} \\ 1 & 1
\end{pmatrix}^{-1} \begin{pmatrix}
  a \\ b
\end{pmatrix}\\
&= \begin{pmatrix}
  \frac{4}{3}\left( \frac{6}{5} \right)^n - \frac{1}{3}\left( \frac{3}{5} \right)^n & \frac{1}{3}\left( \frac{3}{5} \right)^n - \frac{1}{3}\left( \frac{6}{5} \right)^n\\
  \frac{4}{3}\left( \frac{6}{5} \right)^n - \frac{4}{3}\left( \frac{3}{5} \right)^n & \frac{4}{3}\left( \frac{3}{5} \right)^n - \frac{1}{3}\left( \frac{6}{5} \right)^n
\end{pmatrix}\begin{pmatrix}
  a \\ b
\end{pmatrix}\\
&= \begin{pmatrix}
  \frac{1}{3}\left( \frac{6}{5} \right)^n(4a  - b) + \frac{1}{3}\left(\frac{3}{5}\right)^n(b - a)\\
  \frac{1}{3}\left( \frac{6}{5} \right)^n(4a  - b) + \frac{4}{3}\left(\frac{3}{5}\right)^n(b - a)\\
\end{pmatrix}
  \end{align*}

  \noindent
  (ii) \quad As we let $n \to \infty$, $\left( \frac{3}{5} \right)^n \to 0$ and so $\vec{x}(n)$ goes to,
  $$ \begin{pmatrix}
    \frac{1}{3}\left( \frac{6}{5} \right)^n(4a  - b)\\
    \frac{1}{3}\left( \frac{6}{5} \right)^n(4a  - b)\\
  \end{pmatrix} $$
  which then has a ratio of $1 : 1$.\\

  \noindent
  (iii) \quad Hence looking at our $\vec{x}(n)$, the long term growth rate is 1.2 as all the other terms go to 0.
\end{solution}

\newpage
\begin{problem}
  Consider the data points $\{(1, 3),(2, 5),(3, -1)\}$.
  \begin{enumerate}
    \item Find a line of best fit (in the sense of least squares) through the points and find the error. Is this line unique?
    \item Now find a polynomial of degree 2 of best fit and find the error.
    \item Explain how the result in (ii) relates to question 3.
  \end{enumerate}
\end{problem}

\begin{solution}
  We can set up a least squares problem using the points,
  $$ \begin{pmatrix}
    1 & 1 \\
    1 & 2 \\
    1 & 3
  \end{pmatrix} \begin{pmatrix}
    b_0 \\ b_1
  \end{pmatrix} = \begin{pmatrix}
    3 \\ 5 \\ -1
  \end{pmatrix}$$
  We notice that $A$ isn't square and so we are going to multiply both sides by $A^T$ (by Theorem 13.28), and so the problem above is the same as,
  $$ \begin{pmatrix}
    3 & 6 \\ 6 & 14
  \end{pmatrix} \begin{pmatrix}
    b_0 \\ b_1
  \end{pmatrix} = \begin{pmatrix}
    7 \\ 10
  \end{pmatrix}$$
  and this is easily solvable by just left multiplying the inverse of $A^TA$ and we get that,
  $$ \begin{pmatrix}
    b_0 \\ b_1
  \end{pmatrix} = \frac{1}{3}\begin{pmatrix}
    19 \\ -6
  \end{pmatrix} $$
  and hence the least squares line of best fit is, $\displaystyle{\frac{19}{6} - 2x}$. Now we calculate the error,
  $$ \left\| \begin{pmatrix}
    3 \\ 5 \\ -1
  \end{pmatrix} - \begin{pmatrix}
    \frac{13}{3} \\ \frac{7}{3} \\ \frac{1}{3}
  \end{pmatrix}\right\| = \frac{4\sqrt 6}{3}$$
  We also note that as $\displaystyle{\begin{pmatrix}
    1 \\ 1 \\ 1
  \end{pmatrix}}$ and $\displaystyle{\begin{pmatrix}
    1 \\ 2 \\ 3
  \end{pmatrix}}$ are linearly independent this solution is unique.\\

  \noindent
  Now for the degree 2 case, it is actually a lot easier as we get a square matrix in the original problem,
  $$ \begin{pmatrix}
    1 & 1 & 1 \\
    1 & 2 & 4 \\
    1 & 3 & 9
  \end{pmatrix}\begin{pmatrix}
    b_0 \\ b_1 \\ b_2
  \end{pmatrix} \begin{pmatrix}
    3 \\ 5 \\ -1
  \end{pmatrix}$$
  and hence, just left multiplying by $A^{-1}$ we get that,
  $$ \begin{pmatrix}
    b_0 \\ b_1 \\ b_2
  \end{pmatrix} = \begin{pmatrix}
    -7 \\ 14 \\ -4
  \end{pmatrix}$$
  and hence the degree solution is, $p(x) = -7 + 14x - 4x^2$. We can also now calculate error, which we can tell without even doing the calculation as we have three data points and three degrees of freedom and hence this polynomial is going to fit the data exactly and hence error is $0$.\\

  \noindent
  (iii) \quad $T$ is the same transformation as in Q3, as it's just the way we produce a least squares matrix for $x = \{1, 2, 3\}$ and as it's the same transformation it's the same matrix as every transformation matrix is unique.
\end{solution}

\end{document}
