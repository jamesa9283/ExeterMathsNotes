% !TEX root = ../notes.tex

\section{Discrete Time Systems in 1D}
Consider,
$$ N_{t+1} = f(N_t) $$
 and $t \in \Z$. We can say that $N_t$ is the population size at time step $t$, and we now have an evolution function $f: \R \to \R$. Starting at $N_0$, we can generate a trajectory by just recursively applying our map.
 \[\begin{tikzcd}
	{N_0} & {N_1} & {N_2} & \dots & {}
	\arrow["f", from=1-1, to=1-2]
	\arrow["f", from=1-2, to=1-3]
	\arrow["f", from=1-3, to=1-4]
\end{tikzcd}\]
Here is an example, $f(N) = rN$ where $r > 0$ is just our birth rate. We get that $N_t = r^tN_0$. So there are three things that can happen,
\begin{enumerate}
  \item $N_t \to 0$ as $t \to \infty$ if $r < 1$ (extinction)
  \item $N_t \to \infty$ as $t \to \infty$ if $r > 1$ (survival)
  \item $N_t \to N_0$ as $t \to \infty$ if $r = 1$ (survival)
\end{enumerate}

We will be considering maps with a single maximum in $(0,\,\infty)$, so something like an inverted quadratic. Again we will loop at the logistic map,
$$ f(N) = rN\left( 1 - \frac{N}{k} \right) $$

\subsection{Equilibria, Stability and Cobwebbing}
Recall that for a continuous time system, $N(t) = N_{*}$ if $\fa t\ge 0$ if $f(N_*) = 0$. For discrete systems,
$$ N_t = N_* $$
$$ \implies N_{t+1} = N_t = N_* \,\fa t \ge 0$$
and so we just want $f(N_*) = N_*$. We call $N_*$ as fixed point for the function $f$.\\

We can consider cobwebbing to find these Equilibria, we take a initial condition, then draw up to $f$, then project to $f(N) = N$, then project down again to $N_1$. We then repeat this.

% diagram for just -x^2

If we can converge on a fixed point we can think of it as stable, but some fixed points can't be converged upon and so they are unstable. Again, we see that $f'(N_*)$ defined the dynamics. There are four possibilities.
\begin{enumerate}
  \item $0 \le f'(N_*) < 1$, we converge onto a fixed point wherever we start from. So $N_t \to N_0$ and so $N_0$ is stable.
  \item $f'(N_*) > 1$ and so our cobweb just diverges to positive or negative infinity. It diverges monotonically in each direction.
  \item $-1 < f'(N_*)$, here we get a spiral inwards. Here $N_*$ is oscillatory stable. This a non-monotonic convergence.
  \item $f'(N_*) < -1$, here we get an unstable spiral. It diverges to infinity. We say $N_*$ is oscillatory unstable.
\end{enumerate}
To see this formally, we write $N_t = N_* + n_t$ and take some sort of Taylor expansion. Then,
\begin{align*}
  N_* + n_t &= f(N_* + n_t) \\
  &= f(N_*) + f'(n_t)n_t + hot.\\
  &= f'(N_*)n_t
\end{align*}
Ignoring the higher order terms we can write,
$$ n_{t+1} = f'(N_*)n_t $$
and so now we can consider the assymptotic dynamics and we see that all we need to consider is $f'(N_*)$. Hence we see,
\begin{enumerate}
  \item If $|f'(N_*)| < 1$, then $n_t \to 0$ and $N_t \to N_*$ as $t \to \infty$. Hence $N_*$ is a stable fixed point
  \item If $|f'(N_*)| > 1$ and so $n_t \to \infty$ and $N_t \to \infty$. Hence $N_*$ is an unstable fixed point.
  \item If $|f'(N_*)| = 1$, there is a change in stability, called a bifurcation.
\end{enumerate}

If we are interested in the types of bifurcation and the characteristics of each bifurcation, we will consider solutions of different periods. For the bifurcation of the logistic map at $r=3$, we consider period 2 solutions. If we consider these maps, we can find a different type of bifurcation called a period-doubling bifurcation. For each bifurcation we see period $2^n$ solutions. This is where the `classic' bifurcations come from. This is what we call a period-doubling cascade. An interesting thing is when $r = 3.57$, we get $2^n$-period solutions for all $n$, but they are all unstable. This is deterministic chaos.

\section{Reaction Kinetics}
We will consider the Michaelis-Menten Scheme, we will consider an enzyme, $E$, that binds to a substrate $S$ in order to catalyse $S$ into a product $P$.
$$ \ce{S + E <=>[k_1][k_{-1}] C ->[k_2] P + E} $$
We want to know that happens as we increase time. So we want to create systems of differential equations. We use the Law of Mass Action, this states that the rate of a reaction is proportional to the concetration of the reactants. Our reaction scheme has $m$ reactions, $\{R_1, \dots, R_m\}$ and $n$ reactants $\{X_1, \dots, X_n\}$. We denote the concentration of $x_i = [X_i]$. We then define,
\begin{enumerate}
  \item Chemical species vector, $x = \begin{pmatrix}
    x_1 & \dots & x_n
  \end{pmatrix}^T$
  \item Reaction vector, $\vec v(\vec x) = \begin{pmatrix}
    \vec v_1(\vec x) & \dots & \vec v_m(\vec x)
  \end{pmatrix}^T$ where $\vec v_j (\vec x)$ is the rate of $R_j$.
  \item Stoichiometry Matrix, $N = (N_{ij})$ where $N_{ij}$ is the number of molecules of the $X_i$ produced/consumed in $R_j$.
\end{enumerate}
We then obtain the equations for $\vec x(t)$,
$$ \dot{\vec x} = N\vec v(\vec x) $$
Hence we just decompose our equations,
\begin{align}
  \ce{S + E ->[k_1] C} \tag{$R1$} \\
  \ce{C ->[k_{-1}] S + E} \tag{$R2$} \\
  \ce{C ->[k_2] P + E} \tag{$R3$}
\end{align}
and we write $[S] = s$, $[E] = e$ and so on. We want to write,
$$ \vec x = \begin{pmatrix}
  s \\ e \\ c \\ p
\end{pmatrix},\, \vec v(\vec x) = \begin{pmatrix}
  k_1se \\ k_{-1}c \\ k_2c
\end{pmatrix},\, N = \begin{pmatrix}
  -1 & 1 & 0\\
  -1 & 1 & 1 \\
  1 & -1 & -1\\
  0 & 0 & 1
\end{pmatrix} $$
Now we can say,
$$ \dit \begin{pmatrix}
  s \\ e \\ c \\ p
\end{pmatrix} = \begin{pmatrix}
  -1 & 1 & 0\\
  -1 & 1 & 1 \\
  1 & -1 & -1\\
  0 & 0 & 1
\end{pmatrix} \begin{pmatrix}
  k_1se \\ k_{-1}c \\ k_2c
\end{pmatrix} $$
and so we want write,
\begin{align*}
  \dit s &= -k_1se + k_{-1}c \\
  \dit e &= -k_1se + k_{-1}c + k_2c \\
  \dit c &= k_1se - k_{-1}c - k_2c \\
  \dit p &= k_2c
\end{align*}
Now we choose suitable intial conditions, say $s(0) = s_0$, $e(0) = e_0$, $c(0) = p(0) = 0$. Now we can deduce that the last equation can jusr be solved, it doesn't depend on any other equation. Hence we can write,
$$ p(t) = k_2\int_0^t c(s)\,ds $$
and also we can say that $\dot e + \dot c = 0$ and the amount of enzyme is conserved. Hence,
$$ e(t) + c(t) = k\,\fa t  $$
and hence, $e(0) + c(0) = e_0 = k$ and so $e(t) = e_0 - c(t)$ and we can compute the enzyme concentration from $c$. Now we can reduce our equations using this,
\begin{align*}
  \dot s &= -k_1e_0s + (k_1s + k_{-1}c) \\
  \dot c &= k_1e_0s - (k_1s + k_{-1} + k_2)c
\end{align*}
Now we can now simplify these further by non-dimensionalise them. We let $\tau = k_1e_0t$ and then via chain rule,
\begin{align*}
  \di s \tau &= \di s t \di t \tau \\
  &= \frac{1}{k_1e_0}\dot s \\
  &= -s + (s + \frac{k_{-1}}{k_1})\frac{c}{e}
\end{align*}
and so the second we get that,
$$ \di{c}{\tau} = s - (s + \frac{k_{-1} + k_2}{k_1})\frac{c}{e} $$
and we let $v = \frac{c}{e_0}$,
\begin{align*}
  \di s \tau &= -s + (s + \frac{k_{-1}}{k_1})v \\
  \di v \tau &= \frac{s}{e_0}u + (\frac{s}{e_0} - \frac{k_{-1} + k_2}{e_0k_1})v
\end{align*}
Now we let $u = \frac{s}{e_0}$
\begin{align*}
  \di u \tau &= -u + (u + \frac{k_{-1}}{k_1s_0})v\\
  \frac{e_0}{s_0}\di v \tau &= u - (u + \frac{k_{-1}+k_2}{s_0k_1})v
\end{align*}
Now let $\e = \frac{e_0}{s_0}$ and $k = \frac{k_{-1} + k_2}{k_1s_0}$ and $\l = \frac{k_2}{k_1s_0}$ and we get,
\begin{align*}
  \di u \tau &= -u + (u + k - \l)v\\
  \e\di v \tau &= u - (u + k)v
\end{align*}
with initial conditions of $u(0) = 1$ and $v(0) = 0$. In general we have $e_0 << s_0$ and so $\e << 1$. Hence, let $\e = 0$ (the rate of the complex formation is very fast), following $\di v \tau \approx 0$, the complex is essentally at equilibrium. This is referred to as the \textbf{Quasi-Stable State Assumption}, and implies,
$$ v = \frac{u}{u+k} $$
and so,
$$ \di u \tau = -u + (u + k - \l)\frac{u}{u+k} = -\l \frac{u}{u+k} $$
and in dimensionalised form,
$$ \di s t = \frac{V_{max}s}{s + k_m} $$
where $V_{max} = k_2e_0$ and $k_m = \frac{k_{-1} + k_2}{k_1}$. Hence if follows that,
$$ \di p t = \frac{V_{max}s}{s + k_m} $$
this is the Michaelis-Menten equation for product formation. We call the $k_m$ the Michaelis Constant, it is a threshold value.\\

\subsection{Two Binding States - Cooperative Binding}
Now we consider there are two binding states for the substrate and the binding of one substrate molecule fascilitates the binding of the other. Then we can show that,
$$ \di p \tau = \frac{v_{max}s^2}{s^2 + k_m^2} $$
This is the hill function and for $n$ binding sites,
$$ \di p t = \frac{v_{max}s^n}{s^n + k_m^n} $$
The hill functions are used as a template for modelling gene expression.

\subsection{Autocatalysts}
Consider the following reaction,
$$ \ce{A + X ->[k_1] 2X} \qquad \ce{x + Y ->[k_2] 2Y} \qquad \ce{Y ->[k_3] B} $$
where we fix $[A] = a_0$ and the usual setup applies. This scheme is an example of autocatalysis, $X$ and $Y$ are involved in the their own production. We convert the scheme to ODEs as before,
$$ \vec x= \begin{pmatrix}
  x \\ y \\ b
\end{pmatrix}\, \vec v(\vec x) = \begin{pmatrix}
  k_1a_0 x \\ k_2xy \\ k_3y
\end{pmatrix} ,\, N = \begin{pmatrix}
  1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1
\end{pmatrix} $$
and now we get the following,
\begin{align*}
  \dot x &= k_1a_0 x - k_2xy \\
  \dot y &= k_2xy - k_3y \\
  \dot b &= k_3y
\end{align*}
and again we see that $b(t) = k_3 \int_0^t y(s)\,ds$. So we now non-dimensionalise this system, define $\tau = k_1a_0 t$ and then,
\begin{align*}
  \di x \tau &= \di x t \di t \tau \\
  &= x - \frac{k_2}{k_1a_0}xy
\end{align*}
similarly,
$$ \di y \tau = \frac{k_2}{k_1a_0}xy - \frac{k_3}{k_1a_0}y $$
now let $v = \frac{k_2}{k_1a_0}$ and so,
\begin{align*}
  \di x \tau &= x - xv \\
  \di v \tau &= \frac{k_2}{k_1a_0}( \frac{k_2}{k_3}x - v)
\end{align*}
Now finally, we let $u = \frac{k_2}{k_3}x$ and let $\a = \frac{k_3}{k_1a_0}$. Then,
\begin{align*}
  \di u \tau &= u(1 - v) \\
  \di v \tau &= \a v(u - 1)
\end{align*}
This is just the Lotka-Volterra Predator-Prey system. We can write,
\begin{align*}
  \di u v &= \di u \tau \di \tau v \\
  &= \frac{u(1 - u)}{\a v(u - 1)} \\
  \a \frac{u-1}{u}du &= \frac{1-v}{v}dv \\
  C &= \a u + v + \ln(u^\a v)
\end{align*}
We see that for $C \ge 1 + \a$, the solutions are closed curves over $(u, v)$ plane. We get this oscillatory behaviour as both $u$ and $v$ feed into eachother.

\section{Stage Structured Population Models- PPMs}
We start by considering classification by age, here we have one class per year of their life cycle and the discrete time steps are in years. We can represent our population as a population vector
$$ \vec x(t) = \begin{pmatrix}
  x_1(t) & x_2(t) & \dots & x_n(t)
\end{pmatrix}^T $$
where $x_i(t)$ is population at life cycle $i$. A simple age-based odel is the following,
\begin{align*}
  x_1(t+1) &= f_1x_1(t) + \dots + f_nx_n(t) \\
  x_2(t+1) &= p_1x_1(t) \\
  x_3(t+1) &= p_2x_2(t) \\
  & \vdots \\
  x_n(t+1) &= p_{n-1}x_{n-1}(t)
\end{align*}
If wr let $f_i$ be the fecundities in each age class and the $p_i$'s the survival probabilities of each age class. We note that $0 \le p_i \le 1$ and usually the first couple of $f_i$'s are zero ($f_i \ge 0$). We can write the following in vector form,
$$ \vec x(t+1) = \begin{pmatrix}
  f_1 & f_2 & f_3 & \dots & f_n \\
  p_1 & 0 & 0 & \dots & 0 \\
  0 & p_2 & 0 & \dots & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \dots & p_n
\end{pmatrix} \vec x(t)$$
We denote this middle matrix as $L$ as it's a Leslie matrix. We are now going to generalise this model to be a stage structured model. This has the form,
\begin{align*}
  x_1(t+1) &= f_1x_1(t) + \dots + f_nx_n(t) \\
  x_2(t+1) &= g_1x_1(t) + p_2x_2(t) \\
  x_3(t+1) &= g_2x_2(t) + p_3x_3(t) \\
  & \vdots \\
  x_n(t+1) &= g_{n-1}x_{n-1}(t) + p_n(t)x_n(t)
\end{align*}
Again $f_i$'s are the fecundities of each stage class, the $g_i$'s are the growth probability and $p'_i$'s as the stasis probability of each stage class. Again $0 \le p_i, g_i \le 1$ and $f_i \ge 0$. Now we see,
$$ \vec x(t+1) = \begin{pmatrix}
  f_1 & f_2 & f_3 & \dots & 0 & f_n \\
  g_1 & p_2 & 0 & \dots & 0 & 0 \\
  0 & g_2 & p_3 & \dots  & 0 & 0 \\
  0 & 0 & g_3 & \dots  & 0 & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
  0 & 0 & 0 & \dots & g_{n-1} & p_n
\end{pmatrix} \vec x(t)$$
These are examples of the PPM models, these have the general form:
$$ \vec x(t+1) = A\vec x(t) \qquad \vec x(0) = \vec x_0 $$
\begin{eg}
  \textbf{Tulip PPM}\\
  Assume three stage classes: Seeds, Bulbs and Flowers. We assume that seeds can become bulbs and bulbs can become flowers. Bulbs can stay as bulbs and flowers as flowers. Finally flowers produce bulbs and seeds. Hence our PPM is,
  $$ A = \begin{pmatrix}
    0 & 0 & A_{13} \\
    A_{21} & A_{22} & A_{23} \\
    0 & A_{32} & A_{33}
  \end{pmatrix} $$
\end{eg}

\subsection{Assymptotic Dynamics}
We are really interested in assymptotic dynamics, so we consider $\vec x(t+1) = A\vec x(t)$ and we can see that $x(t) = A^t\vec x_0$. Hence we are really interested in what $A^t$ looks like. So we want the eigenvalue decomposition of $A$. Assume that $A$ has eigenvalues of $\{\l_1, \l_2, \dots, \l_n\}$ and eigenvectors, $\{\vec w_1, \vec w_2, \dots, \vec w_n\}$ and now we write,
$$ P = \begin{pmatrix}
  \vec w_1 & \vec w_2 & \dots & \vec w_n
\end{pmatrix} $$
and so,
$$ AP = \begin{pmatrix}
  A\vec w_1 & A\vec w_2 & \dots & A\vec w_n
\end{pmatrix} = \begin{pmatrix}
  \vec w_1 & \vec w_2 & \dots & \vec w_n
\end{pmatrix}\begin{pmatrix}
  \l_1 & 0 & \dots & 0\\
  0 & \l_2 & \dots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \dots & \l_n
\end{pmatrix} $$
and so we can write $AP = P\La$ and so $A = P\La P^{-1}$. Using this similarity condition, we want find that,
$$ A^n = P\La P^{-1}P\La P^{-1} \dots P\La P^{-1} = P\La^n P^{-1} $$
Now we can expand $\vec x_0 = \a_1\vec w_1 + \a_2 \vec w_2 \dots + \a_n + \vec w_2$ and so,
$$ \vec x_0 = P\vec\a , \qquad \vec \a = \begin{pmatrix}
  \a_1 \\ \vdots \\ \a_n
\end{pmatrix} $$
and so $\vec x(t) = A^t\vec x_0 = P\La^t \a$ and
$$ \La^t = \begin{pmatrix}
  \l_1^t && 0 \\
  &\ddots& \\
  0&& \l_n^t
\end{pmatrix} $$
and so we can now write that,
\begin{align*}
  \vec x (t) &= \begin{pmatrix}
    \vec w_1 & \vec w_2 & \dots & \vec w_n
  \end{pmatrix}\begin{pmatrix}
    \l_1^t && 0 \\
    &\ddots& \\
    0&& \l_n^t
  \end{pmatrix}\begin{pmatrix}
    \a_1 \\ \vdots \\ \a_n
  \end{pmatrix} \\
  &= \a_1\l_1^t \vec w_1 + \a_2\l_2^t \vec w_2 + \dots + \a_n\l_n^t\vec w_n
\end{align*}
This is the eigenmode expansion of a solution. We can say that as $t \to \infty$, then there will be a mode that dominates.
\begin{nthm}[Perron-Frobenius Theorem]
  Assume that the PPM $A$ is primitive, ie. for some $k \ge 1$, $(A^k)_{ij} \ge 0$. Then:
  \begin{enumerate}
    \item $A$ has a single eigenvalue $\l_1$ of maximum modulus.
    \item $\l_1$ is real and positive
    \item Both the right $\vec w_1$ and the left eigenvector $\vec v_1$ are both positive.
    $$ A\vec w_1 = \l_1 \vec w_1 \qquad \vec v_1^T A = \l_1\vec v_1^T $$
    \item $\vec v_1$ and $\vec w_1$ are unique left and right eigenvectors.
  \end{enumerate}
\end{nthm}
Thus, for a primitive $A$,
$$ \vec x(t) \to \a_1\l_1^t \vec w_1 \text{ as } t\to \infty $$
or more formally,
$$ \lim_{t \to \infty} \l_1^{-t}\vec x(t) = \a_1\vec w_1 $$
so now only three things can happen assumptotically,
\begin{enumerate}
  \item If $\l_1 > 1$, then $\norm{x(t)} \to \infty$ as $t \to \infty$
  \item If $\l_1 = 1$, then $\vec x(t) \to \a_1\vec w_1$ as $t \to \infty$, so it stabilises
  \item If $\l_1 < 1$, then $\vec x(t) \to \vec 0$ as $t \to \infty$, so we have extinction
\end{enumerate}
Now we can consider the fixed points, $x(t) = \hat x$ for all time. If $\hat{\vec x} = A\hat{\vec x}$ and no eigenvalues are equal to one we see that $|A - I| \ne 0$ so,
$$ \hat{\vec x} = (A - I)\vec 0 = \vec 0 $$
then we have a unique fixed point. Hence, we have the first condition in the theorem it being unstable, the third it being stable and the second a bifurcation.\\

\noindent
What about $\a_1$? We know,
$$ \vec x_0 = \a_1\vec w_1 + \dots + \a_n\vec w_n $$
for $i \ne j$, then $\vec v_i^T \vec w_j = 0$. This can be proved by considering the definitions, multiplying and then manipulating the expressions. So,
\begin{align*}
  \vec v_1^T\vec x_0 &= \a_1\vec v_1^T \vec w_1 + \a_2\vec v_2^T \vec w_2 + \dots + \a_n\vec v_n^T \vec w_n \\
  \vec v_1^T\vec x_0 &= \a_1\vec v_1^T \vec w_1 \\
  \frac{\vec v_1^T\vec x_0}{\vec v_1^T \vec w_1} &= \a_1
\end{align*}
That is the population inertia. \\

\noindent
In summary, when $A$ is primitive,
$$ \lim_{t \to \infty} \l_1^{-t}\vec x(t) = \frac{\vec v_1^T \vec x_0}{\vec v_1^T\vec w_1}\vec w_1 $$
where,
$$ A\vec w_1 = \l_1\vec w_1, \quad \vec v_1^T A = \l_1\vec v_1^T $$
and $\vec w_1$ is the stable state structure and $\vec v_1$ is the reproductive value.\\

\subsection{Issues}

\textbf{We now, for simplicity, drop the subscript $1$, so any $\vec v$, $\vec w$ and $\l$ are the first}.\\

\noindent
\begin{enumerate}
  \item We now consider some target growth rate $\l_T$, and we assume that this to be one. So the question is how can be change $A$ to make $\l = \l_T$.
  \item How can be find bounds on $\frac{\vec v^T \vec x_0}{\vec v^T\vec w}$
\end{enumerate}

\subsubsection{Controlling $\l - \l_T$}
If we are interested in an age-structured model, we have a Leslie matrix. What is the characteristic polynomial of the Leslie matrix?
\begin{align*}
  c_A(s) &= |sI - A| \\
  &= s^n - f_1s^{n-1} - f_2p_1s^{n-2} + \dots + f_np_1 + p_{n-1}
\end{align*}
and we can show this has only one positive root, $\l$. As $s \to \infty$, $c_A(s) \to \infty$. So, for $s \ge 0$ the graph must only touch the $x$ axis once,
%TODO put graph here
Thus for $s \ge 0$:
\begin{itemize}
  \item $c(A) < 0$ if $s > \l$
  \item $c(A) > 0$ if $s < \l$
\end{itemize}
If follows that $\l > \l_T \iff c_A(\l_T) < 0$. In particular,
$$ \l > 1 \iff c_A(1) < 0 $$
This is equivilent to $R(A) > 0$, where,
$$ R(A) = f_1 + f_2p_1 + \dots + f_np_1 + \dots + p_{n-1} $$ and so $\l > 1 \iff R(A) > 1$, growth, and $\l < 1 \iff R(A) < 1$, decline.

\subsubsection{Determining the range of the population inertia}
We know that,
$$ \lim_{t \to \infty} \l^{-t}\vec x(t) = \frac{\vec v^T \vec x_0}{\vec v^T \vec w} $$
we define $\vec x_0 = (x_i)$, $\vec v = (v_i)$, $v_{min} = \min_{i}v_i$ and $v_{max} = \max_i v_i$. Assume wlog,
$$ \sum_{i=1}^n x_{i_0} = 1 $$
then,
\begin{align*}
  \vec v^T x_0 &= \sum_{i=1}^n v_ix_{i_0} \\
  &\le \sum_{i=1}^n v_{max}x_{i_0} \\
  &= v_{max}\sum_{i=1}^n x_{i_0}\\
  &= v_{max}
\end{align*}
and so $\vec v^T \vec x_0 \le v_{max}$ and we can show $v_{min} \le \vec v^T \vec x_0$ and so,
$$ \frac{v_{min}}{\vec v^T w} \le \frac{\vec v^T \vec x_0}{\vec v^T \vec w} \le \frac{v_{max}}{\vec v^T \vec w} $$
These bounds are sharp, these are attained for suitable choices of $\vec x_0$. We just take $\vec x_0$ to have a one where the maximum or minimum value of $\vec v$.

\subsection{Sensitivity Analysis}
We take $A$ and preterb it to $A + \D A$ and so we want to consider $\l(A) \mapsto \l(A + \D A)$. We will firstly consider this through a linear approximation with sensitivity analysis and then transfer function analysis which is exact.

\subsubsection{Sensitivity Analysis}
We want to map $A \mapsto A + \D A$, we then map, each useful quantity from $f(A) \mapsto f(A + \D A)$. Now,
$$ \l(A) = \l(a_{11}, \dots, a_{nn}) $$
and,
$$ \l(A+ \D A) = \l(a_{11} + \D a_{11}, \dots, a_{nn} + \D a_{nn}) $$
and as we have done many times before we approximate this with a Taylor expansion.
\begin{align*}
  \l(A + \D A) = \l(a_{11}, \dots, a_{nn}) + \sum_{i, j = 1}^n \pd{\l}{a_{ij}}(A)\D a_{ij} + \mathcal{O}(2)
\end{align*}
Thus,
$$ \l(A + \D A) = \l(A) + \D\l + \mathcal{O}(2) $$
and similarly,
\begin{align*}
  \vec v (A + \D A) &= \vec v(A) + \D\vec v + \mathcal{O}(2)\\
  \vec w (A + \D A) &= \vec w(A) + \D\vec w + \mathcal{O}(2)\\
\end{align*}
and now exploit that our equations will satisfy the perterbed eigenvalue eignvector equations,
\begin{align*}
  (A + \D A)\vec w(A + \D A) &= \l(A + \D A)\vec w(A + \D A)\\
  (A + \D A + \mathcal{O}(2))(\vec w + \vec{ \D w} + \mathcal{O}(2)) &= (\l(A) + \D\l + \mathcal{O}(2))(\vec w + \vec{\D w} + \mathcal{O}(2))\\
  A\vec w + A\vec{\D w} + \mathcal{O}(2) + \D A\vec w + \D A\vec{\D w} + \mathcal{O}(2) &= \l(A)\vec w + \l(A)\vec{\D w} + \mathcal{O}(2) + \D\l\vec{\D w} + \mathcal{O}(2) \\
  A\vec w + A\vec{\D w} + \D A\vec w &= \l\vec w + \l\vec{\D w} + \D\l\vec{w} + \mathcal{O}(2) \\
  \vec v^T A\vec w + \vec v^T A\vec{\D w} + \vec v^T\D \vec w &= \l\vec v^T\vec w + \l\vec v^T\vec{\D w} + \D\l\vec v^T\vec{w} + \mathcal{O}(2)
\end{align*}
Now $TA\vec w = \l\vec w$ and $\vec v^T A = \l\vec v^T $
\begin{align*}
  \l\vec v^T\vec w + \l\vec v^T \D\vec{ w} + \vec v^T\D A\vec w &= \l\vec v^T\vec w + \l\vec v^T\vec{\D w} + \D\l\vec v^T\vec{w} + \mathcal{O}(2) \\
  \vec v^T\D A\vec w &=\D\l\vec v^T\vec{w} + \mathcal{O}(2)
\end{align*}
Thus,
$$ \D\l = \frac{\vec v^T\D A\vec w}{\vec v^T \vec w} + \mathcal{O}(2) $$
We now assume that $\D A$ is obtained by perturbing one entry $a_{ij}$ ($a_{ij} \mapsto a_{ij} + \D a_{ij}$). Then, \begin{align*}
  \D A &= \D a_{ij} \vec e_i \vec e_j^T \\
  &= \begin{pmatrix}
    0 &&&& 0 \\
    & \ddots &&\iddots& \\
    &&a_{ij}&&\\
    & \iddots &&\ddots&\\
    0 &&&& 0
\end{pmatrix}
\end{align*}
where it is on the $i^{th}$ row and $j^{th}$ column. We now substitute this back in and obtain,
\begin{align*}
  \D \l &= \frac{\vec v^T \vec e_i \vec e_j^T \vec w}{\vec v^T \vec w}\D a_{ij} + \mathcal{O}(2) \\
  &= \frac{v_i w_j}{\vec v^T\vec w}\D a_{ij} + \mathcal{O}(2) \\
  \frac{\D \l}{\D a_{ij}} &= \frac{v_i w_j}{\vec v^T\vec w} + \mathcal{O}(1)
\end{align*}
Now take the limit as $\D a_{ij} \to 0$, we obtain,
$$ \pd{\l}{a_{ij}} = \frac{v_i w_j}{\vec v^T \vec w} $$
These are our terms and so we can find the sensitivity matrix,
$$ S = \left( \pd{\l}{a_{ij}} \right) = \frac{\vec v \vec w^T}{\vec v^T \vec w} $$

\textbf{Summary:} We mapped $A \mapsto \D A$ and $\l(A) \mapsto \l(A + \D A)$ where $\L(A + \D A) = \l(A) + \D\l + \mathcal{O}(2)$ and,
$$ \D \l = \sum_{i=1}^n \pd{\l}{a_{ij}}(A)\D a_{ij} $$
In particular, if we only have one element being perterbed,
$$ \l(A + \D A) = \l(A) + \pd{\l}{a_{ij}}(A)\D a_{ij} + \mathcal{O}(2) $$

\noindent
We want $A + \D A$ with an eigenvalue $\l_T$. Then we can say that,
$$ \l_T = \l + \pd{\l}{a_{ij}}\D a_{ij} + \mathcal{O}(2) $$
Then it follows for small pertibations,
$$ \D a_{ij} \approx \frac{\l_T - \l}{\pd{\l}{a_{ij}}} $$
this is a linear approximation. We can choose the smallest perturbation after calculating the partial derivatives.

\subsubsection{Transfer Function Analysis for PPMs}
The idea here is $a_{ij} \mapsto a_{ij} + p_{ij}$ such that the perturbed eigenvalue is $\l$:
$$ \D A = p_{ij}\vec e_i\vec e_j^T = \begin{pmatrix}
  0 &&&& 0 \\
  & \ddots &&\iddots& \\
  &&p_{ij}&&\\
  & \iddots &&\ddots&\\
  0 &&&& 0
\end{pmatrix}  $$
and again we proceed from the perturbed eigenvalue equation.
\begin{align*}
  (A + \D A)\vec w &= \l \vec w \\
  A\vec w + p_{ij}\vec e_i\vec e_j^T \vec w &= \l I\vec w \\
  (\l I - A)\vec w &= p_{ij}\vec e_i\vec e_j^T \vec w
\end{align*}
Now assume that $|\l I - A|\ne 0$ thus can multiply both sides by $e_j^T (\l I - A)^{-1}$
\begin{align*}
  \vec e_j^T (\l I - A)^{-1} (\l I - A)\vec w &= \vec e_j^T (\l I - A)^{-1}p_{ij}\vec e_i\vec e_j^T \vec w \\
  \vec e_j^T \vec w &= p_{ij} \vec e_j^T (\l I - A)^{-1} \vec e_i (\vec e_j^T \vec w) \\
  1 &= p_{ij} \vec e_j^T (\l I - A)^{-1} \vec e_i \\
  p_{ij} &= \frac{1}{\vec e_j^T (\l I - A)^{-1} \vec e_i} \\
  p_{ij} &= \frac{1}{[(\l I - A)^{-T}]_{ij}}
\end{align*}

\subsection{Transient Dynamics}
\subsubsection{From transient to asymptotic dynamics}
For $\vec x(t + 1) = A\vec x(t)$ where $\vec x(t) = (x_i(t))$ and we can write,
\begin{equation}
  \lim_{t\to \infty} \l^{-t}\vec x(t) = \frac{\vec v^T\vec x(0)}{\vec v^T \vec w}\vec w.\label{equ:asymp}
\end{equation}
We now introduce $N(t) = \norm{\vec x(t)}_1 = \sum_{i=1}^n |x_i(t)| = \sum_{i=1}^n x_i(t) = \vec 1^T \vec x(t)$. Now we consider \ref{equ:asymp} and then dot with $\vec 1$.
\begin{align*}
  \vec 1^T \lim_{t\to \infty} \l^{-t}\vec x(t) &= \vec 1^T \frac{\vec v^T\vec x(0)}{\vec v^T \vec w}\vec w\\
  \lim_{t\to \infty} \l^{-t}\vec 1^T \vec x(t) = \frac{\vec v^T \vec x(0)}{\vec v^T \vec w}\vec 1^T \vec w \\
  \lim_{t\to \infty} \l^{-t}N(t) = \frac{\vec v^T \vec x(0)}{\vec v^T \vec w}\norm{\vec w}_1 \\
  \lim_{t\to \infty} N(t) = \frac{\vec v^T \vec x(0)}{\vec v^T \vec w}\norm{\vec w}_1 \l^t \\
\end{align*}
Asymptotically, three possibilities. We have exponential growth, stasis and exponential decay. What we want to know now is what happens before the asymptotic behaviour? We want some way to bound the possibilities.
\begin{figure}[!ht]
\centering
\resizebox{0.48\textwidth}{!}{\input{./figures/transPPM.pdf_tex}}
\caption{Transient and Asymptotic Dynamics.}
\end{figure}
\noindent

\subsubsection{Bound 1 - Population Amplification and Attenuation}
We will consider maximum and minimum column sums. We know $\vec x(t) = A^t \vec x(0)$. Thus $N(t) = \vec q^t \vec x(t) = \vec 1^T A^t \vec x(0) = \vec 1^T M \vec x(0)$, where $M = A^t$ given $\vec x(0)$ such that $N(0) = \vec 1^T \vec x(0) = 1$. How large or small can $N(t)$ become? \\

\noindent
Assume that $M = (m_{ij})$ is a non-negative matrix and that $\vec x = (x_j)$ is a non-negative vector. Then,
\begin{align*}
  \vec 1^T \vec M \vec x &= \sum_{i,j = 1}^n M_{ij}x_j = \sum_{j=1}^n \left(\sum_{i=1}^n M_{ij}\right)x_j\\
  &\le \sum_{j=1}^n \max_j \left(\sum_{i=1}^n M_{ij} \right)x_j = \max_j \left(\sum_{i=1}^n M_{ij} \right) \sum_{j=1}^n x_j = \left( \max_j \sum_{i=1}^n M_{ij} \right)\vec 1^T \vec x
\end{align*}
We have to show that,
$$ \vec 1^T M\vec x \le \bar c \vec 1^T \vec x $$
where $\bar c = \max_j \sum_{i=1}^n M_{ij} = \sum_{i=1}^n M_{ij_M}$. Similarly we can show that,
$$ \vec 1^T M\vec x \ge \underline c \vec 1^T\vec x $$
where $\underline c = \min_j \sum_{i=1}^n M_{ij} = \sum_{i=1}^n M_{ij_m}$. Taking these together,
\begin{equation}
  \underline x \vec 1^T \vec x \le \vec 1^T M\vec x \le \bar c \vec 1^T \vec x \tag{*}\label{equ:star}
\end{equation}

\noindent
Take $M = A^t$, $\vec x = \vec x(0)$ in \refeq{equ:star}, then,
$$ \underline c \vec 1^T \vec x(0) \le \vec 1^T A^t \vec x(0) \le \bar c \vec 1^T \vec x $$
where $\bar c$ is the maximum column sum of $A^t$, denoted $\rho_t$, and $\underline c$ is the minimum column sum of $A^t$, denoted, $a_t$. Thus,
$$ a_t \le N(t) \le \rho_t \qquad t \ge 0. $$
This is bound 1. We call $a_t$ the minimum attenuation and $\rho_t$ the maximum attenuation.
\begin{figure}[!ht]
\centering
\resizebox{0.48\textwidth}{!}{\input{./figures/bound1.pdf_tex}}
\caption{Bound 1.}
\end{figure}
These bounds are sharp. That is, for a given $t'$, $\vec x(0) = \vec e_{jM}$ (a basis vector), this will give $N(t') = \rho_{t'}$ and $\vec x(0) = \vec e_{jm}$ will give $N(t') = a_{t'}$. In simpler terms, they can be attained.

\noindent
\subsubsection{Bound 2}

Now we multiply on the left by $\vec v^T$ and get, (wlog when we write $\vec v$ and $\l$ we mean $\vec v_1$ and $\l_1$, the dominant eigenvector. )
\begin{align*}
  \vec v^T \vec x(t+1) &= \vec v^T \vec A\vec x(t) \\
  &= \l \vec v^T\vec x(t) \\
  &= \sum v_i x_i(t)
\end{align*}
and so we can write,
$$ v_{\text{min}}N(t) \le \vec v^T \vec x(t) \le v_{\text{max}}N(t) $$
and so we can form another bound,
$$ v_{\text{min}}N(t+1) \le \l v^T_{\max}N(t) $$
and the other way around and so,
$$\frac{v_{\text{min}}}{v_{\text{max}}}\l^t \le N(t) \le \frac{v_{\text{max}}}{v_{\text{min}}}\l^t $$
This is bound 2. This bound isn't sharp, but is a lot easier to calculate.
\subsubsection{Bound 3}
We now look at $\vec x(t) = A^t\vec x(0)$ and we write this as
\begin{align*}
  N(t+1) &= \mathbf{1} A\vec x(t)\\
  &= \begin{pmatrix}
    c_1(A) & \dots & c_n(A)
\end{pmatrix} \vec x(t)
\end{align*}
Now we will get $\underline c(A) N(t) \le N(t+1) \le \bar c(A) N(t)$ and now we iterate and get that,
$$ \underline c(A)^t N(0) \le N(t) \le (\bar c(A))^tN(0). $$
This is bound 3. This only contains values that can be found from $A$.
\subsubsection{Combining Bounds}

We now consider combining these bounds to create one overall bound. We consider bound 2 and 3. These say,
$$ N(t) \ge \frac{v_{\text{min}}}{v_{\text{max}}}\l^t, \qquad N(t) \ge \underline c^t. $$
This gives,
$$ N(t) \ge \max\left(\frac{v_{\text{min}}}{v_{\text{max}}}\l^t, \underline c^t \right) $$
Then bound 1 gives us that $N(t) \ge a_t$, with $N(t) = a_t$ for suitable $\vec x(0)$. Thus,
$$ a_t \ge \max\left(\frac{v_{\text{min}}}{v_{\text{max}}}\l^t, \underline c^t \right).$$
Similarly, follows from bounds 2 and 3 that,
$$ \rho_t \le \min\left( \frac{v_{\text{max}}}{v_{\text{min}}}\l^t, \bar c^t \right) $$
\begin{figure}[!ht]
\centering
\resizebox{0.48\textwidth}{!}{\input{./figures/combinedBounds.pdf_tex}}
\caption{Combined Bounds.}
\end{figure}

\noindent
Where the red lines are the bounds for $a_t$ and $\rho_t$. Further, these then give us bounds for any solutions for all the different initial conditions. That is, we can attain the less accurate bounds from just $A$. Sometimes as $A$ gets complex $A^t$ is very hard to calculate.