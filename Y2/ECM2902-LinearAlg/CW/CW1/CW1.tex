\documentclass{article}

% Packages
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{float}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{shapes.geometric, positioning, arrows, intersections, backgrounds,arrows.meta}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}
\usepackage{esint}
\usepackage{tikz-3dplot}

\tikzset {b/.code = {\pgfsetadditionalshadetransform{ \pgftransformshift{\pgfpoint{0 bp } { 0 bp }  }  \pgftransformrotate{0 }  \pgftransformscale{2 }  }}}
\pgfdeclarehorizontalshading{a}{150bp}{rgb(0bp)=(1,1,1);
rgb(37.5bp)=(1,1,1);
rgb(45.427829197474885bp)=(0.88,0.88,0.88);
rgb(50bp)=(0.95,0.95,0.95);
rgb(62.5bp)=(0.96,0.96,0.96);
rgb(100bp)=(0.96,0.96,0.96)}
\tikzset{every picture/.style={line width=0.75pt}}

% Macros
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\sub}{\subset}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\uvec}[1]{\hat{\underline{\textbf{#1}}}}
\newcommand{\veci}{\bm{\hat{\imath}}}
\newcommand{\vecj}{\bm{\hat{\jmath}}}
\newcommand{\veck}{\bm{\hat{k}}}
\newcommand{\vecn}{\underline{\mathbf{\hat{n}}}}
\newcommand{\e}{\varepsilon}
\renewcommand{\th}{\vartheta}
\newcommand{\de}{\delta}
\renewcommand{\k}{\kappa}
\newcommand{\p}{\Phi}
\renewcommand{\a}{\alpha}
\renewcommand{\b}{\beta}
\newcommand{\pa}{\partial}
\newcommand{\kd}{\delta_{i, j}}
\newcommand{\at}{\e_{i, j, k}}
\newcommand{\nab}{\underline{\nabla}}
\newcommand{\grad}{{\nab}\, f}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\fd}[2]{\frac{d #1}{d #2}}
\renewcommand{\div}{\nab \cdot}
\newcommand{\curl}{\nab \times}
\newcommand{\br}{\underline{\varrho}}
\newcommand{\tbt}[4]{\begin{pmatrix} #1 & #2 \\ #3 & #4\end{pmatrix}}
\newcommand{\vect}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\newcommand{\vecth}[3]{\begin{pmatrix} #1 \\ #2 \\ #3 \end{pmatrix}}
\newcommand{\thbth}[9]{\begin{pmatrix} #1 & #2 & #3 \\ #4 & #5 & #6 \\ #7 & #8 & #9\end{pmatrix}}
\newcommand{\iid}{\thbth 1 0 0 0 1 0 0 0 1}
\newcommand{\id}{\tbt 1 0 0 1}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\DeclareMathOperator{\spn}{span}

%ToC stuff
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}

\tcbuselibrary{theorems}
\newtcbtheorem[number within=section]{theorem}{Theorem}%
{colback=green!5,colframe=green!35!black,fonttitle=\bfseries}{th}
\newtcbtheorem[number within=section]{definition}{Definition}%
{colback=blue!5,colframe=blue!35!black,fonttitle=\bfseries}{def}

\title{Linear Algebra - Coursework 1}
\author{James Arthur}

\begin{document}
\maketitle

\begin{problem}
  Determine whether the following subsets of real vector spaces are subspaces. Justify your answers.
  \begin{enumerate}
    \item $T = \{\vec x \in \R^3 : 3x_1 + 4x_2 + x_3 = 0\}$
    \item $\displaystyle{U = \{\vec x \in \R^2 : x_1 + x_2 = 1\}}$
    \item $V = \{ f \in C(\R) : \int_0^1 f(x)\,dx = 3 \}$
    \item $W = \{A \in M_{n \times n}(\R) : Tr(A) = 0\}$
  \end{enumerate}
\end{problem}
\begin{solution}
  i) We firstly have the space $T = \{\vec x \in \R^3 : 3x_1 + 4x_2 + x_3 = 0\}$, by Lemma 2.35 of the notes, we must have that $\vec 0_v \in T$, $T$ is closed under addition and scalar multiplication (smul).\\
  Firstly $\vec 0_v = (0, 0, 0)$, which is in $T$ as, $\displaystyle{3(0) + 4(0 + 0 = 0)}$. For closure under addition, take arbitrary $\vec u, \vec v \in T$ and let $\vec w = \vec u + \vec v$. Now,
  \begin{align*}
    3w_1 + 4w_2 + w_3 &= 3(u_1 + v_1) + 4(u_2 + v_2) + (u_3 + v_3)\\
    &= 3u_1 + 3v_1 + 4u_2 + 4v_2 + w_1 + w_2\\
    &= (3u_1 + 4u_2 + u_3) + (3v_1 + 4v_2 + v_3)\\
    &= 0 + 0 && \text{ as $\vec u, \vec v \in T$}\\
    &= 0 && \text{Hence, $\vec w \in T$}
  \end{align*}
  Hence, $T$ is closed under addition. For smul, we can do something similar, take an arbitrary $u \in T$ and $a \in \R$. Let there be a vector $\vec w = a\vec u$. Now,
  \begin{align*}
    3w_1 + 4w_2 + w_3 &= 3(au_1) + 4(au_2) + au_3\\
    &= a(3u_1 + 4u_2 + u_3)\\
    &= a \cdot 0 && \text{as $\vec u \in T$}\\
    &= 0 && \text{Hence, $a\vec u \in T$}
  \end{align*}
  Hence we have that $T$ is closed under smul and $T$ is a subspace of $\R^3$. A less formal way to see this would be to say that $T$ is a plane passing through the origin and hence it has to have the required properties.\\

  \noindent
  ii) For $\displaystyle{U = \{\vec x \in \R^2 : x_1 + x_2 = 1\}}$, it suffices to notice that there isn't a zero vector, for the zero vector in $\R^2$ doesn't satisfy $x_1 + x_2 = 1$ as by definition, $x_1 + x_2 = 0$. So $U$ is not a subspace of $\R^2$.\\

  \noindent
  iii) $V = \{ f \in C(\R) : \int_0^1 f(x)\,dx = 3 \}$ falls for a very similar problem. If there was a zero function in the space ($f(x) = 0$), then we know that $\displaystyle{\int_0^1 0\,dx = 0}$. Hence it is excluded from the space. $V$ is not a subspace of $C(\R)$\\

  \noindent
  iv) $W = \{A \in M_{n \times n}(\R) : Tr(A) = 0\}$, we can use the fact that $M_{n \times n}(\R)$ is a vector space as we proved in the lecture notes. Here we have some extra constraints. So it suffices to prove that, $\vec 0_{n \times n} \in W$, as $Tr(\vec 0_{n\times n}) = 0$ this is quickly proved. Next we can take $A, B \in M_{n\times n}(\R)$ and prove that,
  $$ Tr(A) + Tr(B) = Tr(A + B) $$
  This is again pretty simple, let the traces of the matrices be,
  $$ Tr(A) = a_1 + a_2 + \dots a_n \qquad Tr(B) = b_1 + b_2 + \dots + b_n $$
  and as $A, B \in W$, by definition $Tr(A) = Tr(B) = 0$. Hence we need to show that, $Tr(A + B) = 0$. We can do this by just noticing that,
  \begin{align*}
    Tr(A + B) &= a_1 + b_1 + \dots + a_n + b_n\\
    &= (a_1 + \dots + a_n) + (b_1 + \dots + b_n) \\
    &= 0 + 0 \\
    &= 0
  \end{align*}
  $$ Tr(A + B) = a_1 + b_1 + \dots + a_n + b_n = (a_1 + \dots + a_n) + (b_1 + \dots + b_n) = 0 + 0 = 0  $$
  Hence $W$ is closed under addition. To prove closure under smul, it again suffices to show that, if $a \in \R$ and $B \in M_{n\times n}(\R)$
  $$ aTr(B) = Tr(aB) $$
  This can be quickly shown by taking the scalar multiple of a matrix $B$, we have that $aB = \{ab_{i, j} : i, j \in {1, \dots, n}\}$, hence we can now just say that,
  \begin{align*}
    Tr(aB) &= ab_1 + \dots + ab_n\\
    &= a(b_1 + \dots + b_n)\\
    &= aTr(B)
  \end{align*}
  Hence $W$ is a subspace of $M_{n\times n}(\R).$\footnote{Interesting fact! We have now also proved that the $Tr(A)$ is a linear transformation!}
\end{solution}

\newpage
\begin{problem}
  Determine whether the following lists of vectors are linearly independent, whether they are spanning sets and whether they are bases,
  \begin{enumerate}
    \item In $\R^2$, the vectors $[\vec v_1, \vec v_2, \vec v_3]$ where,
    $$ \vec v_1 = \vect {-1} 3 \qquad \vec v_2 = \vect 2 {-1} \qquad \vec v_3 = \vect 4 1 $$
    \item  In $\C^3$ (considered as a complex vector space), the vectors $[\vec v_1, \vec v_2, \vec v_3]$ where,
    $$ \vec v_1 = \vecth {2 +3i} 1 3 \qquad \vec v_2 = \vecth 1 0 1 \qquad \vec v_3 = \vecth i 2 {1+i} $$
    \item In $M_{2\times 2}(\R)$, the matrices $[A_1, A_2, A_3]$ where,
    $$ A_1 = \tbt 1 5 {-1} 0 \qquad A_2 = \tbt 1 {-4} 4 1 \qquad A_3 = \tbt 2 1 3 1 $$
    \item ) In $ P_2(\R)$, the polynomials $[p_1, p_2, p_3]$ where,
    $$ p_1(x) = 2 + x - 2x^2 \qquad p_2(x) = 3 - x + x^2 \qquad p_3(x) = -1 + 2x - 3x^2$$
  \end{enumerate}
\end{problem}
\begin{solution}
  We are going to firstly take the vectors and place them into a matrix. By Lemma 5.4 in the notes, if we have a pivot in every row and column after gaussian elimination then they are linearly independent, spanning and a basis. If there is only pivots in the columns or rows then they are only linearly independent or spanning respectively. (Lemma 4.8 / Lemma 3.12)\\

  \noindent
  i) We form our matrix,
  $$ \begin{pmatrix}
    -1 & 2 & 4 \\ 3 & -1 & 1
  \end{pmatrix}  $$
  which when we preform gaussian elimination, we get,
  $$ \begin{pmatrix}
    1 & 0 & \frac{6}{15}\\ 0 & 1 & \frac{13}{5}
  \end{pmatrix} $$
  which then tells us that they are not linearly indendent as we don't have pivots in every column and so hence not a basis but they are spanning as we have a pivot in every row.\\

  \noindent
  ii) We form another matrix for our complex vectors,
  $$ \thbth {2+3i} 1 i 1 0 2 2 1 {1+i} $$
  which we can preform gaussian elimination on and get that,
  $$ \iid $$
  Which we can look at quickly and tell that it's a basis, and hence linearly independent and spanning of $\C^3$.\\\newpage
  \noindent
  iii) We first need to turn these matrices into co-ordinate vectors as the set of $M_{2\times 2} \cong \R^4$ (Example 8.2). Hence take the standard basis and rewite as coordinate vectors,
  $$ [m_0, m_1, m_2, m_3] = \left[\tbt 1 0 0 0, \tbt 0 1 0 0, \tbt 0 0 1 0, \tbt 0 0 0 1 \right] $$
  and hence we get the matrix,
  $$ \begin{pmatrix}
    1 & 1 & 2 \\ 5 & -4 & 1 \\ -1 & 4 & 3\\ 0 & 1 & 1
  \end{pmatrix} $$
  which reduces to,
  $$ \begin{pmatrix}
    1 & 0 & 1 \\
    0 & 1 & 1 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{pmatrix} $$
  Hence this set of vectors is not spanning or linearly independent as there are two pivots in the second row and third column.\\

  \noindent
  iv) We can do this one by a very similar method to (iii), so transform into coordinate vectors with the following mapping,
  $$ [l_0, l_1, l_2] = [1, x, x^2] $$
  Hence we can rewrite,
  $$ p_1 \to \begin{pmatrix}
    2 \\ 1 \\ -2
  \end{pmatrix} \quad p_2 \to \begin{pmatrix}
    3 \\ -1 \\ 1
  \end{pmatrix} \quad p_2 \to \begin{pmatrix}
    -1 \\ 2 \\ -3
  \end{pmatrix}$$
  and hence produce the matrix and reduce it,
  $$ \thbth 2 3 {-1} 1 {-1} 2 {-2} 1 3  \to \thbth 1 0 1 0 1 {-1} 0 0 0$$
  which again isn't linearly independent or spanning as there is an empty row and two pivots in the second row and third column.
\end{solution}

\newpage
\begin{problem}
  Let $V$ be a vector space. Prove that a list of vectors $[\vec u,\,\vec v]$ from $V$ is linearly independent if and only if the list $[\vec u + \vec v,\, \vec u - \vec v]$ is linearly independent.
\end{problem}
\begin{solution}
  Suppose that $[\vec u,\, \vec v]$ is a list of linearly independent vectors. That means,
  $$ a_1\vec u + a_2\vec v = \vec 0_v \iff a_1 = a_2 = 0 $$
  Now as $a_1, a_2 \in \R$, we can write them as, $a_1 = b_1 + b_2$ and $a_2 = b_1 - b_2$ as the vectors, $\displaystyle{\begin{pmatrix} 1 \\ 1\end{pmatrix}}$ and $\displaystyle{\begin{pmatrix} 1 \\ -1\end{pmatrix}}$ are linearly independent and spanning, hence that system of equations is not constrained, and so $a_1$ and $a_2$ can still be any real numbers. So we plug in our change of variables,
  $$ (b_1 + b_2)\vec u + (b_1 - b_2)\vec v = 0 \iff b_1 = b_2 = 0 $$
  and hence we can rewrite the first part of that equation to get,
  $$ b_1(\vec u + \vec v) + b_2(\vec u - \vec v) = 0 \iff b_1 = b_2 = 0 $$
  Hence if $[\vec u,\, \vec v]$ are linearly independent so is $[\vec u + \vec v,\, \vec u - \vec v]$\\

  \noindent
  Now suppose that $[\vec u + \vec v,\, \vec u - \vec v]$ is a list of linearly independent vectors. That means,
  $$ \a_1(\vec u + \vec v) + \a_2(\vec u - \vec v) = 0 \iff \a_1 = \a_2 = 0 $$
  Hence, we can rewite this proposition as,
  $$ (\a_1 + \a_2)\vec u + (\a_1 - \a_2)\vec v = 0 \iff \a_1 = \a_2 = 0 $$
  Now as the reals are closed under addition, and hence subtraction, we can let $\a_1 + \a_2 = \b_1$ and $\a_1 - \a_2 = \b_2$ and so,
  $$ \b_1\vec u_1 + \b_2\vec u_2 = 0 \iff \b_1 = \b_2 = 0 $$
  Hence if $[\vec u + \vec v, \,\vec u - \vec v]$ is linearly independent then so is $[\vec u,\, \vec v]$.\\

  \noindent
  Pulling together both halfs of the proof, $[\vec u, \vec v]$ is linearly independent if and only if $[\vec u + \vec v,\, \vec u - \vec v]$ is also linearly independent.
\end{solution}\qed

\newpage
\begin{problem}
  Let $V = \R^3$. Let,
  $$ \vec u_1 = \vecth 0 0 1 \qquad \vec w_1 = \vecth 0 1 1 \qquad \vec w_2 = \vecth 1 {-1} 0$$
  and,
  $$ U = \spn (\{\vec u_1\}) \qquad W = \spn(\{\vec w_1, \vec w_2\}) $$
  \begin{enumerate}
    \item Show that $V = U \oplus W$.
    \item Determine the actions of the projections $P_U$ onto $U$ along $W$, and $P_W$ onto $W$ along $U$ on an arbitrary vector $\vec v = (v_1, v_2, v_3) \in V$ . Hence write down the matrices $[P_U]_E^E$ and $[P_W]_E^E$, where $E$ is the standard basis in $\R^3$.
  \end{enumerate}
\end{problem}
\begin{solution}
  i) To prove that $V = U \oplus W$, we must show that $V = U + W$ and then $U \cup W = \{\vec 0_v\}$. To show that $V = U + W$, it suffices to show that $[\vec u_1,\, \vec w_1,\, \vec w_2]$ is a spanning set for $\R^3$. So we create a matrix and do gaussian elimiation,
  $$ \thbth 0 0 1 0 1 {-1} 1 1 0 \to \iid $$
  Hence the vectors are a spanning set, in fact they are a basis for $\R^3$. Hence, $V = U + W$. Now given that these set of vectors are linearly independent, there is no linear combination of $[\vec w_1,\, \vec w_2]$ that is equal to $\vec u_1$. Hence we can say that $U \cup W = \{0_v\}$ and that $V = U \oplus W$.\\

  \noindent
  ii) We can rewrite $[\vec u_1,\, \vec w_1,\, \vec w_2]$ in an augmented matrix with repsect to a general vector and row reduce it,
  $$ \begin{pmatrix}[ccc|c]
   0 & 0 & 1 & v_1 \\  0 & 1 & -1 & v_2\\ 1 & 1 & 0 & v_3 \\
 \end{pmatrix} \to \begin{pmatrix}[ccc|c]
  1 & 0 & 0 & v_3 - v_2 - v_1 \\  0 & 1 & 0 & v_1 + v_2 \\ 0 & 0 & 1 & v_1 \\
  \end{pmatrix} $$
  We can now go and find the projections from this matrix,
  $$ P_U = (v_3 - v_2 - v_1)\begin{pmatrix}0 \\ 0 \\ 1\end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ v_3 - v_2 - v_1\end{pmatrix} $$
  $$ P_W = (v_2 + v_1)\begin{pmatrix}0 \\ 1 \\ 1\end{pmatrix} + v_1 \begin{pmatrix}1 \\ -1 \\ 0\end{pmatrix} = \begin{pmatrix} v_1 \\ v_2 \\ v_2 + v_1\end{pmatrix} $$
  and we can now write out the matrices for this transformations.
  $$ [P_U]_E^E = \thbth 0 0 0 0 0 0 {-1} {-1} 1  \qquad [P_W]_E^E = \thbth 1 0 0 0 1 0 1 1 0$$
\end{solution}

\newpage
\begin{problem}
  Let $V$ and $W$ be 3-dimensional real vector spaces with ordered bases $P = [v_1,\, v_2,\, v_3]$ and $Q = [w_1,\, w_2,\, w_3]$ respectively. Let $T : V \to W$ be a linear transformation. The matrix of $T$ with respect to $P$ and $Q$ is,
  $$ [T]_P^Q = \thbth 3 1 2 1 5 6 4 9 5 $$
  Find the matrix $[T]_{Q'}^{P'}$ of $T$ with respect to the bases $P' = [v_1',\, v_2',\, v_3']$ and $Q' = [w_1',\, w_2',\, w_3']$ where,
  \begin{align*}
    \vec v_1' &= \vec v_1 + \vec v_2\\
    \vec v_2' &= \vec v_2 + \vec v_3\\
    \vec v_3' &= -2\vec v_1 + \vec v_3
  \end{align*}
  and
  \begin{align*}
    \vec w_1' &= \vec w_1 + 2\vec w_2 - 3\vec w_3\\
    \vec w_2' &= \vec w_1\\
    \vec w_3' &= \vec w_3 + \vec w_2
  \end{align*}
\end{problem}
\begin{solution}
  We can use Corrolary 7.14 and so we can write $[T]_{P'}^{Q'}$ as,
  $$ [T]_{P'}^{Q'} = [id_W]_Q^{Q'}[T]_P^Q[id_v]_{P'}^P $$
  and so we need to calculate $[id_W]_Q^{Q'}$ and $[id_V]_{P'}^{P}$. We can get the following quickly,
  $$ [id_V]_{P'}^P = \thbth 1 0 {-2} 1 1 0 0 1 1 \qquad [id_W]_{Q'}^Q = \thbth 1 1 0 2 0 1 {-3} 0 1 $$
  and by Corollary 7.13, we now have $[id_W]_Q^{Q'} = ([id_W]_{Q'}^Q)^{-1}$ and so we can write,
  \begin{align*}
    [T]_{P'}^{Q'} &= \thbth 1 0 {-2} 1 1 0 0 1 1 \thbth 3 1 2 1 5 6 4 9 5 \thbth 1 1 0 2 0 1 {-3} 0 1 ^{-1} \\
    &= \frac{1}{5}\thbth {-7} {-3} 7 {27} {18} {-27} {44} {61} 6
  \end{align*}
\end{solution}






\end{document}
